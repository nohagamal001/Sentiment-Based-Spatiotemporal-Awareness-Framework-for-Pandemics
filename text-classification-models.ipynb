{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,string\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import accuracy_score,classification_report,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wordEmbeddings_and_classificationModels import prepare_data_for_word_vectors,building_word_vector_model,\\\n",
    "classification_model,cnn_classification_model,lstm_classification_model,bilstm_classification_model,\\\n",
    "padding_input,Embed,ELMoEmbedding,data_prep_ELMo,Classification_model_with_ELMo\n",
    "\n",
    "current_path=os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_dict(json_set):\n",
    "    for k,v in json_set.items():\n",
    "        if v == \"True\":\n",
    "            json_set[k]= True\n",
    "        elif v == \"False\":\n",
    "            json_set[k]=False\n",
    "        else:\n",
    "            json_set[k]=v\n",
    "    return json_set\n",
    "\n",
    "with open(\"config.json\",\"r\") as f:\n",
    "    params_set = json.load(f)\n",
    "params_set = json_to_dict(params_set)\n",
    "\n",
    "\n",
    "with open(\"model_params.json\", \"r\") as f:\n",
    "    model_params = json.load(f)\n",
    "model_params = json_to_dict(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params[\"loss\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = params_set[\"option\"]\n",
    "options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option Embedding Parameter:\n",
    "- 0  : Word2vec, \n",
    "- 1  : gensim Fastext, \n",
    "- 2  : Fasttext 2018, \n",
    "- 3  : GloVe, \n",
    "- 4  : pre-trained Word2vec, \n",
    "- 5  : word2vec + POS.\n",
    "- 6  : glove + POS\n",
    "- 7  : gensim fasttext + POS\n",
    "- 8  : Fasttext 2018 + POS\n",
    "- 9  : Elmo only \n",
    "- 10 : Elmo + POS \n",
    "\n",
    "- 11 : Elmo + word2vec\n",
    "- 12 : Elmo + pos + word2vec\n",
    "- 13 : Elmo + glove\n",
    "- 14 : Elmo + fattest gensim\n",
    "- 15 : Elmo + pos + glove\n",
    "- 16 : Elmo + pos + fasttext gensim\n",
    "\n",
    "- 17 : Elmo + character embedding\n",
    "- 18 : Elmo + POS + character embedding\n",
    "- 19 : Elmo + Glove + POS + character embedding\n",
    "- 20 : Elmo + word2vec + POS + character embedding\n",
    "- 21 : Elmo + fasttext gensim + pos + character embedding\n",
    "\n",
    "\n",
    "#### Neural Network Cell Parameter: (For options 0,1,2,3,4)\n",
    "- 0 for simple 2 layers MLP\n",
    "- 1 for lstm\n",
    "- 2 for bilstm\n",
    "- 3 for cnn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\n",
    "# K.clear_session()\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "class Histories(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # K.clear_session()\n",
    "        # tf.reset_default_graph()\n",
    "        self.aucs = []\n",
    "        self.losses = []\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        decay = self.model.optimizer.decay\n",
    "        iterations = self.model.optimizer.iterations\n",
    "        lr_with_decay = lr / (1. + decay * K.cast(iterations, K.dtype(decay)))\n",
    "        print(K.eval(lr_with_decay))\n",
    "        #K.clear_session()\n",
    "        #tf.reset_default_graph()\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "\n",
    "histories = Histories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data,X,y,max_len) :\n",
    "\n",
    "    results = set()\n",
    "    train_data['text'].str.lower().str.split().apply(results.update)\n",
    "    words = results  # set(text_to_word_sequence(text))\n",
    "    vocab_size = len(words) + 1\n",
    "    params_set[\"vocab_size\"] = vocab_size\n",
    "    for option in options:\n",
    "        print(\"Option:\",option)\n",
    "        for nn_cell in  model_params[\"nn_cell\"]:\n",
    "            sentences_as_words,sentences,word_ix = prepare_data_for_word_vectors(X)\n",
    "            print(\"Sentences loaded\")\n",
    "            model_wv = building_word_vector_model(option,sentences_as_words,params_set[\"embed_dim\"],\n",
    "                                               params_set[\"workers\"],params_set[\"window\"],train_data,X,y)\n",
    "            print(\"Word vector model built\")\n",
    "            x_train, x_test, y_train, y_test = train_test_split(sentences, y, \n",
    "                                                                test_size=params_set[\"split_ratio\"], \n",
    "                                                                random_state=42)\n",
    "            print(\"Data split done\")\n",
    "            x_train_pad,x_test_pad = padding_input(x_train,x_test,params_set[\"max_len\"])\n",
    "\n",
    "            embed = Embed(params_set[\"vocab_size\"],params_set[\"embed_dim\"],\n",
    "              params_set[\"pos_embed_dim\"],params_set[\"max_len\"],True)\n",
    "\n",
    "            if option in [0,1,2,3,4]:\n",
    "                if nn_cell == 0:\n",
    "                    print(\"Building simple MLP model\")\n",
    "                    model = classification_model(params_set[\"embed_dim\"],x_train_pad,x_test_pad,\n",
    "                                 y_train,y_test,params_set[\"vocab_size\"],word_ix,model_wv,\n",
    "                                 params_set[\"trainable_param\"],params_set[\"option\"])\n",
    "                    print(\"Simple MLP Model is built\")\n",
    "                elif nn_cell == 1:\n",
    "                    print(\"Building LSTM model\")\n",
    "                    model = lstm_classification_model(params_set[\"embed_dim\"],x_train_pad,x_test_pad,\n",
    "                                 x_train,y_train,y_test,\n",
    "                                 params_set[\"vocab_size\"],word_ix,model_wv,\n",
    "                                 params_set[\"trainable_param\"],params_set[\"option\"])\n",
    "                    print(\"LSTM Model is built\")\n",
    "\n",
    "                elif nn_cell == 2:\n",
    "                    print(\"Building BiLSTM model\")\n",
    "                    model = bilstm_classification_model(params_set[\"embed_dim\"],x_train_pad,x_test_pad,\n",
    "                                 x_train,y_train,y_test,\n",
    "                                 params_set[\"vocab_size\"],word_ix,model_wv,\n",
    "                                 params_set[\"trainable_param\"],params_set[\"option\"])\n",
    "                    print(\"BiLSTM Model is built\")\n",
    "\n",
    "                elif nn_cell == 3:\n",
    "                    print(\"Building CNN model\")\n",
    "                    model = cnn_classification_model(X,y,x_train_pad,x_test_pad,\n",
    "                                 y_train,y_test,params_set[\"vocab_size\"])\n",
    "\n",
    "                    print(\"CNN Model is built\")\n",
    "\n",
    "                print(model.summary())\n",
    "                print(\"Traning Model...\")\n",
    "                model.fit(x_train_pad, y_train, epochs= model_params[\"epochs\"],batch_size=model_params[\"batch_size\"], verbose=1, \n",
    "                          validation_data=(x_test_pad, y_test),callbacks=[histories])\n",
    "                \n",
    "                model_name = ''\n",
    "                if option == 0:\n",
    "                    model_name = 'Word2vec_Model'\n",
    "                elif option == 1:\n",
    "                    model_name = 'Gensim_Fastext_Model'\n",
    "                elif option == 2:\n",
    "                    model_name = 'Fasttext_2018_Model'\n",
    "                elif option == 3:\n",
    "                    model_name = 'GloVe_Model'\n",
    "                elif option == 4:\n",
    "                    model_name = 'pre_trained_Word2vec_Model'\n",
    "                \n",
    "                #save model\n",
    "                model.save(model_name + '.h5')\n",
    "                \n",
    "                predictions = model.predict(x_test_pad)\n",
    "                \n",
    "                print(\"=======================================================================================================\")\n",
    "\n",
    "            elif option in [5,6,7,8]:\n",
    "                inp_seq,sent_emb = embed.embed_sentences(word_ix,model_wv,False,x_train_pad)\n",
    "                pos_enc = embed.tag_pos(sentences_as_words)\n",
    "                print(\"POS encoded\")\n",
    "                x_train_pos, x_test_pos, _, _ = train_test_split(pos_enc, y, \n",
    "                                                                 test_size=params_set[\"split_ratio\"], \n",
    "                                                                 random_state=42)\n",
    "                x_train_pos_pad,x_test_pos_pad = padding_input(x_train_pos,x_test_pos,params_set[\"max_len\"])\n",
    "                print(\"POS padded\")\n",
    "\n",
    "                inp_pos,pos_embed = embed.embed_pos(x_train_pos_pad)\n",
    "                \n",
    "                model_name = ''\n",
    "                if option == 5 :\n",
    "                    print(\"Building a combined Word2vec & POS model\")\n",
    "                    model_name = 'Word2vec_POS_Model'\n",
    "                elif option == 6:\n",
    "                    print(\"Building a combined Glove & POS model\")\n",
    "                    model_name = 'Glove_POS_Model'\n",
    "                elif option == 7:\n",
    "                    print(\"Building a combined Fasttext & POS model\")\n",
    "                    model_name = 'Fasttext_POS_Model'\n",
    "                elif option == 8:\n",
    "                    print(\"Building a combined Fasttext 2018 + POS model\")\n",
    "                    model_name = 'Fasttext_2018_POS_Model'\n",
    "                    \n",
    "                combined_model = Embed.pos_model_build(inp_seq,inp_pos,sent_emb,pos_embed,x_train_pad,x_train_pos_pad,y_train,\n",
    "                                    model_params[\"epochs\"],model_params[\"batch_size\"],x_test_pad,x_test_pos_pad,y_test)\n",
    "                if option == 5 :\n",
    "                    print(\"A combined Word2vec & POS model is built\")\n",
    "                if option == 6 :\n",
    "                    print(\"A combined Glove & POS model is built\")\n",
    "                if option == 7 :\n",
    "                    print(\"A combined Fasttext & POS model is built\")\n",
    "                if option == 8 :\n",
    "                    print(\"A combined Fasttext 2018 & POS model is built\")\n",
    "                print(combined_model.summary())\n",
    "                \n",
    "                print(\"Traning Combined Model...\")\n",
    "                combined_model.fit([x_train_pad, x_train_pos_pad], y_train, \n",
    "                                   epochs=model_params[\"epochs\"],batch_size=model_params[\"batch_size\"],\n",
    "                                  validation_data=([x_test_pad, x_test_pos_pad], y_test),callbacks=[histories])\n",
    "                \n",
    "                #save model\n",
    "                combined_model.save(model_name + '.h5')\n",
    "                \n",
    "                predictions = combined_model.predict([x_test_pad, x_test_pos_pad])\n",
    "                \n",
    "                print(\"=======================================================================================================\")\n",
    "\n",
    "            elif option == 9 :\n",
    "                train_text,train_label,test_text,test_label = data_prep_ELMo(x_train,y_train,x_test,y_test,max_len,word_ix)\n",
    "                print(\"Building Elmo model\")\n",
    "                elmo_model = Classification_model_with_ELMo(train_text,train_label,\n",
    "                                       test_text,test_label,\n",
    "                                        vocab_size,\n",
    "                                       epochs=model_params[\"epochs\"],\n",
    "                                       batch_size=model_params[\"batch_size\"])\n",
    "                print(\"Elmo model is built\")\n",
    "                print(elmo_model.summary())\n",
    "\n",
    "                print(\"Traning Elmo Model...\")\n",
    "                elmo_model.fit(train_text,train_label,epochs=model_params[\"epochs\"],batch_size=model_params[\"batch_size\"],\n",
    "                          validation_data=(test_text,test_label),callbacks=[histories])\n",
    "                \n",
    "                #save model\n",
    "                elmo_model.save('Elmo_Model.h5')\n",
    "\n",
    "                predictions = elmo_model.predict(test_text)\n",
    "                \n",
    "                print(\"=======================================================================================================\")\n",
    "\n",
    "            elif option == 10 :\n",
    "                train_text,train_label,test_text,test_label = data_prep_ELMo(x_train,y_train,x_test,y_test,max_len,word_ix)\n",
    "                elmo_model = Classification_model_with_ELMo(train_text,train_label,\n",
    "                                       test_text,test_label,\n",
    "                                        vocab_size,\n",
    "                                       epochs=model_params[\"epochs\"],\n",
    "                                       batch_size=model_params[\"batch_size\"])\n",
    "                \n",
    "                inp_seq,sent_emb = embed.embed_sentences(word_ix,elmo_model,False,x_train_pad)\n",
    "                pos_enc = embed.tag_pos(sentences_as_words)\n",
    "                print(\"POS encoded\")\n",
    "                x_train_pos, x_test_pos, _, _ = train_test_split(pos_enc, y, \n",
    "                                                                 test_size=params_set[\"split_ratio\"], \n",
    "                                                                 random_state=42)\n",
    "                x_train_pos_pad,x_test_pos_pad = padding_input(x_train_pos,x_test_pos,params_set[\"max_len\"])\n",
    "                print(\"POS padded\")\n",
    "\n",
    "                inp_pos,pos_embed = embed.embed_pos(x_train_pos_pad)\n",
    "\n",
    "                print(\"Building a combined Elmo & POS model\")\n",
    "                combined_model = Embed.pos_model_build(inp_seq,inp_pos,sent_emb,pos_embed,x_train_pad,x_train_pos_pad,y_train,\n",
    "                                    model_params[\"epochs\"],model_params[\"batch_size\"],x_test_pad,x_test_pos_pad,y_test)\n",
    "\n",
    "                print(\"A combined Elmo & POS model is built\")\n",
    "                print(combined_model.summary())\n",
    "\n",
    "                print(\"Traning Combined Elmo & POS Model...\")    \n",
    "                combined_model.fit([x_train_pad, x_train_pos_pad], y_train, \n",
    "                                   epochs=model_params[\"epochs\"],\n",
    "                                   batch_size=model_params[\"batch_size\"],\n",
    "                                   validation_data=([x_test_pad, x_test_pos_pad], y_test),callbacks=[histories])\n",
    "            \n",
    "                # evaluate the model\n",
    "                print(\"Evaluate ..\")\n",
    "                loss, accuracy = model.evaluate(x_train_pad, y_train, verbose=1)\n",
    "                print('Accuracy: %f' % (accuracy*100))\n",
    "                \n",
    "                # save model\n",
    "                combined_model.save('Elmo_POS_Model.h5')\n",
    "\n",
    "                # make predictions from the model\n",
    "                print(\"Make predicttions ..\")                \n",
    "                predictions = combined_model.predict([x_test_pad, x_test_pos_pad])\n",
    "\n",
    "            \n",
    "            # predictions = [int(0) if i < 0.5 else int(1) for i in predictions]\n",
    "            predicted_class = np.argmax(predictions, axis=1)\n",
    "            # print(\"predictions argmax: \",predictions)\n",
    "            predicted_class = predicted_class.tolist()\n",
    "            y_test = [ np.argmax(i) for i in y_test]\n",
    "            # print(\"y_test classes: \",y_test)\n",
    "            # print(\"predicted_class: \",predicted_class)\n",
    "            \n",
    "            print(\"F1-Score: \",f1_score(y_test, predicted_class, average='macro')  )\n",
    "            print(\"Accuracy: \",accuracy_score(y_test, predicted_class))\n",
    "            print(\"Classification Report: \",classification_report(y_test,predicted_class))\n",
    "            print(\"=======================================================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# covid_tweets_with_sentiments_2021 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_train_data_path = \"covid_tweets_with_sentiments_2021-08-26.csv\"\n",
    "twitter_df = pd.read_csv(twitter_train_data_path, index_col = 0)\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_ids = {\"positive\":1, \"negative\":-1,\"neutral\":0}\n",
    "ids_sentiments = {id:sent for sent,id in sentiments_ids.items()}\n",
    "twitter_df = twitter_df[twitter_df['sentiment'].notna()]\n",
    "twitter_df = twitter_df[twitter_df['text'].notna()]\n",
    "twitter_df['sentiment'] = twitter_df['sentiment'].replace(sentiments_ids)\n",
    "twitter_df['sentiment'] = twitter_df['sentiment'].astype(int)\n",
    "twitter_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "X = np.array(list(twitter_df.text.iloc[:100]))\n",
    "y = np.array(twitter_df.sentiment.iloc[:100])\n",
    "y_binary = to_categorical(y)\n",
    "\n",
    "max_len = len(np.max(twitter_df.text.iloc[:100]))\n",
    "train_model(twitter_df,X,y_binary,max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### APIs for covid-19 cases stats. per country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_world_population_countries_names():\n",
    "    url = \"https://world-population.p.rapidapi.com/allcountriesname\"\n",
    "    headers = {\n",
    "        'x-rapidapi-key': \"134117ae79msh40bb2931f9c7e4ap1aa445jsn8d1982670b09\",\n",
    "        'x-rapidapi-host': \"world-population.p.rapidapi.com\"\n",
    "        }\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "    return response.json() \n",
    "\n",
    "\n",
    "def get_world_population_per_country(country):\n",
    "    url = \"https://world-population.p.rapidapi.com/population\"\n",
    "    querystring = {\"country_name\":country}\n",
    "    headers = {\n",
    "        'x-rapidapi-key': \"134117ae79msh40bb2931f9c7e4ap1aa445jsn8d1982670b09\",\n",
    "        'x-rapidapi-host': \"world-population.p.rapidapi.com\"\n",
    "        }\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_covid19api_all_countries():\n",
    "    url = \"https://api.covid19api.com/countries\"\n",
    "    payload={}\n",
    "    headers = {}\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload, timeout=10)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_covid19api_stats_country(country , from_date , to_date ):\n",
    "    url = \"https://api.covid19api.com/total/country/{}?from={}&to={}\".format(country , from_date , to_date )\n",
    "    payload={}\n",
    "    headers = {}\n",
    "    response = requests.request(\"GET\", url, headers=headers, data = payload )\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_covid_observer_countries():\n",
    "    url = \"https://covid.observer/us/#countries\"\n",
    "    response = requests.request(\"GET\", url, timeout=10)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    countries_list_element = soup.find_all(\"div\",{\"class\":\"countries-list\"})[2]\n",
    "    \n",
    "    result = {}\n",
    "    for a_tag in countries_list_element.find_all(\"a\"):\n",
    "        result[a_tag.text] = \"https://covid.observer\" + a_tag['href']\n",
    "    \n",
    "    return result\n",
    "\n",
    "# get topN similar sentences for a given sentence using Cosine Similarity\n",
    "def most_similar_sentence(sentence, all_sentences):\n",
    "\n",
    "#     print(\"Given sentence: \",sentence)\n",
    "    max_cosine_sim = 0.5\n",
    "    most_sim_sentence = ''\n",
    "    most_sim_sentence_index = 0\n",
    "    \n",
    "    for index, sim_sentence in enumerate(all_sentences):\n",
    "        cosine = Cosine(2)\n",
    "        s0 = sentence\n",
    "        s1 = sim_sentence\n",
    "        p0 = cosine.get_profile(s0)\n",
    "        p1 = cosine.get_profile(s1)\n",
    "        if p1 and cosine.similarity_profiles(p0, p1) > max_cosine_sim:\n",
    "            max_cosine_sim = cosine.similarity_profiles(p0, p1)\n",
    "            most_sim_sentence = sim_sentence\n",
    "            most_sim_sentence_index = index\n",
    "            \n",
    "    sentences_cosine_similarities = (s0,most_sim_sentence,max_cosine_sim)\n",
    "#     print(\"Most similar sentence: {}\".format(most_sim_sentence).encode('utf-8').strip())\n",
    "\n",
    "    return (most_sim_sentence, max_cosine_sim, most_sim_sentence_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get covid cases dataframe for specific country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling covid cases for United Kingdom\n",
      "Crawling covid cases for United States of America\n",
      "Crawling covid cases for Uruguay\n",
      "Crawling covid cases for Uzbekistan\n",
      "Crawling covid cases for Vanuatu\n",
      "Crawling covid cases for Venezuela\n",
      "Crawling covid cases for Viet Nam\n",
      "Crawling covid cases for Yemen\n",
      "Crawling covid cases for Zambia\n",
      "Crawling covid cases for Zimbabwe\n",
      "len(all_country_stat_dfs):  216\n"
     ]
    }
   ],
   "source": [
    "# get world countries\n",
    "all_countries = get_covid_observer_countries()\n",
    "\n",
    "# get most similar country\n",
    "# sim_country = most_similar_sentence(country, all_countries.keys())[0]\n",
    "\n",
    "\n",
    "all_country_stat_dfs = []\n",
    "\n",
    "for country in all_countries.keys():\n",
    "\n",
    "    print(\"Crawling covid cases for {}\".format(country))\n",
    "    \n",
    "    # get API url for the current country\n",
    "    url = all_countries[country]\n",
    "\n",
    "    # make a get request\n",
    "    page = requests.get(url)\n",
    "\n",
    "    # declare a BeautifulSoup object from request content\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "    # text mining on BeautifulSoup object\n",
    "    table_data = soup.find('table')\n",
    "    headers = []\n",
    "    for i in table_data.find_all('th'):\n",
    "        title = i.text\n",
    "        headers.append(title)\n",
    "\n",
    "    # creatr a DataFrame\n",
    "    specific_country_stat_df = pd.DataFrame(columns = headers)\n",
    "\n",
    "    for j in table_data.find_all('tr')[1:]:\n",
    "        row_data = j.find_all('td')\n",
    "        row = [tr.text for tr in row_data]\n",
    "        length = len(specific_country_stat_df)\n",
    "        specific_country_stat_df.loc[length] = row\n",
    "\n",
    "\n",
    "    specific_country_stat_df.columns = ['date','confirmed_cases','daily_growth','recovered_cases',\n",
    "                                         'fatal_cases','active_cases','recovery_rate','mortality_rate',\n",
    "                                         'affected_population','confirmed_per_1000','died_per_1000']\n",
    "\n",
    "    specific_country_stat_df['index'] = specific_country_stat_df.index\n",
    "\n",
    "    specific_country_stat_df['country'] = country\n",
    "    \n",
    "    all_country_stat_dfs.append(specific_country_stat_df)\n",
    "    \n",
    "print(\"len(all_country_stat_dfs): \", len(all_country_stat_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114173, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>confirmed_cases</th>\n",
       "      <th>daily_growth</th>\n",
       "      <th>recovered_cases</th>\n",
       "      <th>fatal_cases</th>\n",
       "      <th>active_cases</th>\n",
       "      <th>recovery_rate</th>\n",
       "      <th>mortality_rate</th>\n",
       "      <th>affected_population</th>\n",
       "      <th>confirmed_per_1000</th>\n",
       "      <th>died_per_1000</th>\n",
       "      <th>index</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sep 9</td>\n",
       "      <td>153,840</td>\n",
       "      <td>0.1 %</td>\n",
       "      <td>0</td>\n",
       "      <td>7,157</td>\n",
       "      <td>146,683</td>\n",
       "      <td>0.0 %</td>\n",
       "      <td>4.7 %</td>\n",
       "      <td>0.4 %</td>\n",
       "      <td>4.04</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sep 8</td>\n",
       "      <td>153,736</td>\n",
       "      <td>0.1 %</td>\n",
       "      <td>0</td>\n",
       "      <td>7,151</td>\n",
       "      <td>146,585</td>\n",
       "      <td>0.0 %</td>\n",
       "      <td>4.7 %</td>\n",
       "      <td>0.4 %</td>\n",
       "      <td>4.04</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sep 7</td>\n",
       "      <td>153,626</td>\n",
       "      <td>0.1 %</td>\n",
       "      <td>0</td>\n",
       "      <td>7,144</td>\n",
       "      <td>146,482</td>\n",
       "      <td>0.0 %</td>\n",
       "      <td>4.7 %</td>\n",
       "      <td>0.4 %</td>\n",
       "      <td>4.04</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sep 6</td>\n",
       "      <td>153,534</td>\n",
       "      <td>0.1 %</td>\n",
       "      <td>0</td>\n",
       "      <td>7,141</td>\n",
       "      <td>146,393</td>\n",
       "      <td>0.0 %</td>\n",
       "      <td>4.7 %</td>\n",
       "      <td>0.4 %</td>\n",
       "      <td>4.04</td>\n",
       "      <td>0.19</td>\n",
       "      <td>3</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sep 5</td>\n",
       "      <td>153,375</td>\n",
       "      <td>0.0 %</td>\n",
       "      <td>0</td>\n",
       "      <td>7,127</td>\n",
       "      <td>146,248</td>\n",
       "      <td>0.0 %</td>\n",
       "      <td>4.6 %</td>\n",
       "      <td>0.4 %</td>\n",
       "      <td>4.03</td>\n",
       "      <td>0.19</td>\n",
       "      <td>4</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    date confirmed_cases daily_growth recovered_cases fatal_cases  \\\n",
       "0  Sep 9         153,840        0.1 %               0       7,157   \n",
       "1  Sep 8         153,736        0.1 %               0       7,151   \n",
       "2  Sep 7         153,626        0.1 %               0       7,144   \n",
       "3  Sep 6         153,534        0.1 %               0       7,141   \n",
       "4  Sep 5         153,375        0.0 %               0       7,127   \n",
       "\n",
       "  active_cases recovery_rate mortality_rate affected_population  \\\n",
       "0      146,683         0.0 %          4.7 %               0.4 %   \n",
       "1      146,585         0.0 %          4.7 %               0.4 %   \n",
       "2      146,482         0.0 %          4.7 %               0.4 %   \n",
       "3      146,393         0.0 %          4.7 %               0.4 %   \n",
       "4      146,248         0.0 %          4.6 %               0.4 %   \n",
       "\n",
       "  confirmed_per_1000 died_per_1000 index      country  \n",
       "0               4.04          0.19     0  Afghanistan  \n",
       "1               4.04          0.19     1  Afghanistan  \n",
       "2               4.04          0.19     2  Afghanistan  \n",
       "3               4.04          0.19     3  Afghanistan  \n",
       "4               4.03          0.19     4  Afghanistan  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_country_stat_dataframe = pd.concat(all_country_stat_dfs)\n",
    "print(all_country_stat_dataframe.shape)\n",
    "all_country_stat_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_country_stat_dataframe.to_csv(\"all_country_covid_cases_dataframe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
