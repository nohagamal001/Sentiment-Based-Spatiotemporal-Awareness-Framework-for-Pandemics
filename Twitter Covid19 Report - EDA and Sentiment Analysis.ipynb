{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section consists of various section of geo analysis of data\n",
    "## **Content**\n",
    "\n",
    "1. [Importing Libraries](#1)\n",
    "2. [Loading Data](#2)\n",
    "3. [Preprocessing Data](#3)\n",
    "4. [Data Viualization](#4)\n",
    "5. [Tweet Text Analysis](#5)\n",
    "6. [Sentiment analysis](#6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip3 install -U pip\n",
    "# !pip3 install -r requirements.txt\n",
    "# !sudo apt install python3-dev\n",
    "# !sudo apt-get install libproj-dev proj-data proj-bin  \n",
    "# !sudo apt-get install libgeos-dev  \n",
    "# !sudo apt-get install -y python3-pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from collections import Counter\n",
    "from datetime import date\n",
    "import os\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "import json\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from nltk.corpus import stopwords\n",
    "import geoplot\n",
    "from geopy import Nominatim\n",
    "import folium\n",
    "import mapclassify\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import plotly.express as px \n",
    "import plotly.graph_objs as go \n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from folium.plugins import HeatMapWithTime, TimestampedGeoJson\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from iso3166 import countries\n",
    "import calmap\n",
    "from geopy.geocoders import Nominatim\n",
    "from tqdm import tqdm\n",
    "import country_converter as coco\n",
    "import time\n",
    "import pymongo\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "import matplotlib.ticker as ticker\n",
    "from similarity.cosine import Cosine\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import Binarizer, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "\n",
    "\n",
    "# For Parallel Processing\n",
    "import swifter\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize()\n",
    "\n",
    "import matplotlib.style as style \n",
    "style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(sum(map(ord, 'calmap')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = pymongo.MongoClient()\n",
    "mongo_db = mongo_client[\"twitter\"]\n",
    "mongo_collection = mongo_db[\"tweets\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load world cities dataset\n",
    "cities = pd.read_csv('worldcities.csv')\n",
    "print(cities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tweets dataset from MongoDB\n",
    "# full_dataframe = pd.DataFrame(mongo_collection.find()[:50000])\n",
    "\n",
    "# Load old tweets dataset from csv file\n",
    "# full_dataframe = pd.read_csv(\"All_Tweets.csv\")\n",
    "full_dataframe = pd.read_csv(\"covid19_tweets.csv\")\n",
    "# full_dataframe = full_dataframe[[\"Text\"]]\n",
    "# full_dataframe.columns = [\"text\"]\n",
    "\n",
    "full_dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tweet(tweet):\n",
    "    new_tweet = {}\n",
    "    new_tweet['user_name'] = tweet['user']['screen_name']\n",
    "    new_tweet['user_location'] = tweet['user']['location']\n",
    "    new_tweet['hashtags'] = tweet['entities']['hashtags']\n",
    "    new_tweet['is_retweet'] = tweet['retweeted']\n",
    "    new_tweet['source'] = tweet['source']\n",
    "    new_tweet['user_description'] = tweet['user']['description']\n",
    "    new_tweet['user_created'] = tweet['user']['created_at']\n",
    "    new_tweet['user_followers'] = tweet['user']['followers_count']\n",
    "    new_tweet['user_friends'] = tweet['user']['friends_count']\n",
    "    new_tweet['user_favourites'] = tweet['user']['favourites_count']\n",
    "    new_tweet['user_verified'] = tweet['user']['verified']\n",
    "    new_tweet['date'] = tweet['created_at']\n",
    "    new_tweet['text'] = tweet['full_text']\n",
    "    new_tweet['tweet_id'] = tweet['id']\n",
    "    if tweet['coordinates']:\n",
    "        new_tweet['lat'] =  tweet['coordinates']['coordinates'][0]  \n",
    "        new_tweet['long'] = tweet['coordinates']['coordinates'][1]  \n",
    "    else:\n",
    "        new_tweet['lat'] = 'None'\n",
    "        new_tweet['long'] = 'None'\n",
    "    new_tweet['geo_enabled'] = tweet['user']['geo_enabled']\n",
    "    new_tweet['tweet_place'] = tweet['place']\n",
    "    \n",
    "    return new_tweet\n",
    "\n",
    "\n",
    "\n",
    "# rows = [filter_tweet(row) for index, row in full_dataframe.iterrows()]\n",
    "# full_dataframe = pd.DataFrame(rows)\n",
    "# df = full_dataframe.drop_duplicates(subset='tweet_id', keep=\"last\")\n",
    "\n",
    "df = full_dataframe.drop_duplicates(subset='text', keep=\"last\")\n",
    "\n",
    "del full_dataframe\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "row_num, col_num = df.shape\n",
    "print(f'There are {row_num} rows and {col_num} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "def clean_tweets(text):\n",
    "    '''\n",
    "    Utility function to clean tweet text by removing links, special characters ..etc\n",
    "    using simple regex statements.\n",
    "    '''\n",
    "    \n",
    "    text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", str(text)).split())\n",
    "    # Remove RT\n",
    "    text = re.sub(\"RT @[\\w]*:\",\"\", text)\n",
    "    # Remove citations\n",
    "    text = re.sub(\"@[\\w]*:\",\"\", text)\n",
    "    # Remove hyperlinks\n",
    "    text = re.sub(\"https?://[A-Za-z0-9./]\",\"\", text)\n",
    "    text = re.sub(\"https?:\\/\\/.*\\/\\w*\",\"\", text)\n",
    "    # Remove linebreak, tab, return\n",
    "    text = re.sub('[\\n\\t\\r]+', ' ', text)\n",
    "    # Remove tickers\n",
    "    text = re.sub('\\$\\w*', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub('[' + string.punctuation + ']+', '', text)\n",
    "    # Remove quotes\n",
    "    text = re.sub('\\&*[amp]*\\;|gt+', '', text)\n",
    "    # Remove via with blank\n",
    "    text = re.sub('via+\\s', '', text)\n",
    "    # Remove multiple whitespace\n",
    "    text = re.sub('\\s+\\s+', ' ', text)\n",
    "    # Remove multiple whitespace\n",
    "    text = re.sub('\\s+\\s+', ' ', text)\n",
    "    # Remove HashTags \n",
    "    text = re.sub('\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+', ' ', text)\n",
    "    # Remove Smileys\n",
    "    text = re.sub('[:=]+(|o|O| )+[D\\)\\]]+[\\(\\[]+[pP]+[doO/\\\\]+[\\(\\[]+(\\^_\\^|)', ' ', text)\n",
    "\n",
    "    return text.lower().strip()\n",
    "\n",
    "\n",
    "def get_tweet_sentiment(tweet):\n",
    "    '''\n",
    "    Utility function to classify sentiment of passed tweet\n",
    "    using textblob's sentiment method\n",
    "    '''\n",
    "    if type(tweet) is type(None):\n",
    "        return \"None\"\n",
    "    \n",
    "    # create TextBlob object of passed tweet text\n",
    "    analysis = TextBlob(tweet)\n",
    "    # set sentiment\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "\n",
    "def regularExpression(textToFilter):\n",
    "    filteredTweet = []\n",
    "    retweetPattern = 'RT|@RT'\n",
    "    urlPattern = 'https://[a-zA-Z0-9+&@#/%?=~_|!:,.;]*'\n",
    "\n",
    "    for textLine in textToFilter:\n",
    "        tweet = re.sub(retweetPattern,'',textLine)\n",
    "        tweet = re.sub(urlPattern,'',tweet)\n",
    "        filteredTweet.append(tweet)\n",
    "    return filteredTweet\n",
    "\n",
    "\n",
    "def nltkTokenizer(textToTokenize):\n",
    "    filteredSentence = []\n",
    "    usersPattern = re.compile('@[a-zA-Z0-9]*',re.UNICODE)\n",
    "    hashtagPattern = re.compile('#[a-zA-Z0-9]*',re.UNICODE)\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    for textLine in textToTokenize:\n",
    "        words = re.sub(usersPattern,'',textLine)\n",
    "        words = re.sub(hashtagPattern,'',words)\n",
    "        words = word_tokenize(words)\n",
    "        for w in words:\n",
    "            if w not in stop_words and w not in '@' and w not in '#':\n",
    "                filteredSentence.append(w)\n",
    "    return filteredSentence\n",
    "\n",
    "\n",
    "def tweet_to_words(raw_tweet):\n",
    "    tweet = ''.join(c for c in raw_tweet if c not in string.punctuation)\n",
    "    tweet = re.sub('((www\\S+)|(http\\S+))', 'urlsite', tweet)\n",
    "    tweet = re.sub(r'\\d+', 'contnum', tweet)\n",
    "    tweet = re.sub(' +',' ', tweet)\n",
    "    words = tweet.lower().split()                             \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "                 \n",
    "    meaningful_words = [w for w in words if not w in stops] \n",
    "    return( \" \".join( meaningful_words ))\n",
    "    \n",
    "    \n",
    "def users(tweet):\n",
    "    user = []\n",
    "    usersPattern = re.compile('@[a-zA-Z0-9]*',re.UNICODE)\n",
    "    \n",
    "    for t in tweet:\n",
    "        u = re.findall(usersPattern,t)\n",
    "        user.append(u)\n",
    "    return user\n",
    "\n",
    "\n",
    "def split_into_tokens(Text):\n",
    "    return TextBlob(Text).words\n",
    "\n",
    "\n",
    "def split_into_lemmas(Text):\n",
    "    Text = Text.lower()\n",
    "    words = TextBlob(Text).words\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    return [word.lemma for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning tweets text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"cleaned_text\"] = df[\"text\"].apply(lambda tweet: clean_tweets(tweet))\n",
    "df[\"cleaned_text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Mising Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"country\"] = np.NaN\n",
    "user_location = df['user_location'].fillna(value='').str.split(',')\n",
    "user_location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Countries of users Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = cities['lat'].fillna(value = '').values.tolist()\n",
    "lng = cities['lng'].fillna(value = '').values.tolist()\n",
    "country = cities['country'].fillna(value = '').values.tolist()\n",
    "\n",
    "# Getting all alpha 3 codes into  a list\n",
    "world_city_iso3 = []\n",
    "for c in cities['iso3'].str.lower().str.strip().values.tolist():\n",
    "    if c not in world_city_iso3:\n",
    "        world_city_iso3.append(c)\n",
    "        \n",
    "# Getting all alpha 2 codes into  a list    \n",
    "world_city_iso2 = []\n",
    "for c in cities['iso2'].str.lower().str.strip().values.tolist():\n",
    "    if c not in world_city_iso2:\n",
    "        world_city_iso2.append(c)\n",
    "        \n",
    "# Getting all countries into  a list        \n",
    "world_city_country = []\n",
    "for c in cities['country'].str.lower().str.strip().values.tolist():\n",
    "    if c not in world_city_country:\n",
    "        world_city_country.append(c)\n",
    "\n",
    "# Getting all amdin names into  a list\n",
    "world_states = []\n",
    "for c in cities['admin_name'].str.lower().str.strip().tolist():\n",
    "    world_states.append(c)\n",
    "\n",
    "\n",
    "# Getting all cities into  a list\n",
    "world_city = cities['city'].fillna(value = '').str.lower().str.strip().values.tolist()\n",
    "world_city[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for each_loc in range(len(user_location)):\n",
    "    ind = each_loc\n",
    "    each_loc = user_location[each_loc]\n",
    "    \n",
    "    for each in each_loc:\n",
    "        each = each.lower().strip()\n",
    "        if each in world_city:\n",
    "            order = world_city.index(each)\n",
    "            df['country'][ind] = country[order].lower()\n",
    "            continue\n",
    "        if each in world_states:\n",
    "            order= world_states.index(each)\n",
    "            df['country'][ind] = country[order].lower()\n",
    "            continue\n",
    "        if each in world_city_country:\n",
    "            order = world_city_country.index(each)\n",
    "            df['country'][ind] = world_city_country[order].lower()\n",
    "            continue\n",
    "        if each in world_city_iso2:\n",
    "            order = world_city_iso2.index(each)\n",
    "            df['country'][ind] = world_city_country[order].lower()\n",
    "            continue\n",
    "        if each in world_city_iso3:\n",
    "            order = world_city_iso3.index(each)\n",
    "            df['country'][ind] = world_city_country[order].lower()\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "print(\"Current date:\", datetime.date.today())\n",
    "\n",
    "# saving datafrme\n",
    "df.to_csv(\"covid_tweets_{}.csv\".format(datetime.date.today()), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valid Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Number of valid Tweets Available: ', df['country'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Countries with Most Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_per_country = df['country'].str.lower().dropna()\n",
    "tw = tweet_per_country.value_counts().rename_axis('Country').reset_index(name='Tweet Count')\n",
    "print(tw)\n",
    "plt.rcParams['figure.figsize'] = (15,10)\n",
    "plt.title('Top 10 Countries with Most Tweets',fontsize=15)\n",
    "sns.set_palette(\"husl\")\n",
    "ax = sns.barplot(y=tw['Country'].head(10),x=tw['Tweet Count'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 Countries with Least Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_per_country = df['country'].str.lower().dropna()\n",
    "tw = tweet_per_country.value_counts().rename_axis('Country').reset_index(name='Tweet Count')\n",
    "print(tw)\n",
    "plt.rcParams['figure.figsize'] = (15,10)\n",
    "plt.title('10 Countries with Least Tweets',fontsize=15)\n",
    "sns.set_palette(\"husl\")\n",
    "ax = sns.barplot(y=tw['Country'][-9:],x=tw['Tweet Count'][-9:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min and Max Dates Between The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"date\"].min())\n",
    "print(df[\"date\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 15 Countries with Most Tweets Diffrent Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_graph_03 = px.bar(x='Tweet Count',y='Country',data_frame=tw[:15],color='Country')\n",
    "country_graph_03.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see percent of NaNs for every column. We will visualize only columns with at least 1 missed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "missed = pd.DataFrame()\n",
    "missed['column'] = df.columns\n",
    "\n",
    "missed['percent'] = [round(100* df[col].isnull().sum() / len(df), 2) for col in df.columns]\n",
    "missed = missed.sort_values('percent')\n",
    "missed = missed[missed['percent']>0]\n",
    "\n",
    "fig = px.bar(\n",
    "    missed, \n",
    "    x='percent', \n",
    "    y=\"column\", \n",
    "    orientation='h', \n",
    "    title='Missed values percent for every column (percent > 0)', \n",
    "    height=400, \n",
    "    width=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see top 40 users by number of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = df['user_name'].value_counts().reset_index()\n",
    "ds.columns = ['user_name', 'tweets_count']\n",
    "ds = ds.sort_values(['tweets_count'])\n",
    "\n",
    "fig = px.bar(\n",
    "    ds.tail(40), \n",
    "    x=\"tweets_count\", \n",
    "    y=\"user_name\", \n",
    "    orientation='h', \n",
    "    title='Top 40 users by number of tweets', \n",
    "    width=800, \n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df, ds, on='user_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see most popular users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = df.sort_values('user_followers', ascending=False)\n",
    "data = data.drop_duplicates(subset='user_name', keep=\"first\")\n",
    "data = data[['user_name', 'user_followers', 'tweets_count']]\n",
    "data = data.sort_values('user_followers')\n",
    "\n",
    "fig = px.bar(\n",
    "    data.tail(40), \n",
    "    x=\"user_followers\", \n",
    "    y=\"user_name\", \n",
    "    color='tweets_count',\n",
    "    orientation='h', \n",
    "    title='Top 40 users by number of followers', \n",
    "    width=800, \n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And most friendly users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = df.sort_values('user_friends', ascending=False)\n",
    "data = data.drop_duplicates(subset='user_name', keep=\"first\")\n",
    "data = data[['user_name', 'user_friends', 'tweets_count']]\n",
    "data = data.sort_values('user_friends')\n",
    "\n",
    "fig = px.bar(\n",
    "    data.tail(40), \n",
    "    x=\"user_friends\", \n",
    "    y=\"user_name\", \n",
    "    color = 'tweets_count',\n",
    "    orientation='h', \n",
    "    title='Top 40 users by number of friends', \n",
    "    width=800, \n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how coronavirus affect to new users creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['user_created'] = pd.to_datetime(df['user_created'])\n",
    "df['year_created'] = df['user_created'].dt.year\n",
    "data = df.drop_duplicates(subset='user_name', keep=\"first\")\n",
    "data = data[data['year_created']>1970]\n",
    "data = data['year_created'].value_counts().reset_index()\n",
    "data.columns = ['year', 'number']\n",
    "\n",
    "fig = px.bar(\n",
    "    data, \n",
    "    x=\"year\", \n",
    "    y=\"number\", \n",
    "    orientation='v', \n",
    "    title='User created year by year', \n",
    "    width=800, \n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from chart coronavirus increases the number of new twitter users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see top 40 most popular locations by the number of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = df['user_location'].value_counts().reset_index()\n",
    "ds.columns = ['user_location', 'count']\n",
    "ds = ds[ds['user_location']!='NA']\n",
    "ds = ds.sort_values(['count'])\n",
    "\n",
    "fig = px.bar(\n",
    "    ds.tail(40), \n",
    "    x=\"count\", \n",
    "    y=\"user_location\", \n",
    "    orientation='h', title='Top 40 user locations by number of tweets', \n",
    "    width=800, \n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also we can see the pie plot for the full picture about users locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pie_count(data, field, percent_limit, title):\n",
    "    \n",
    "    data[field] = data[field].fillna('NA')\n",
    "    data = data[field].value_counts().to_frame()\n",
    "\n",
    "    total = data[field].sum()\n",
    "    data['percentage'] = 100 * data[field]/total    \n",
    "\n",
    "    percent_limit = percent_limit\n",
    "    otherdata = data[data['percentage'] < percent_limit] \n",
    "    others = otherdata['percentage'].sum()  \n",
    "    maindata = data[data['percentage'] >= percent_limit]\n",
    "\n",
    "    data = maindata\n",
    "    other_label = \"Others(<\" + str(percent_limit) + \"% each)\"\n",
    "    data.loc[other_label] = pd.Series({field:otherdata[field].sum()}) \n",
    "    \n",
    "    labels = data.index.tolist()   \n",
    "    datavals = data[field].tolist()\n",
    "    \n",
    "    trace=go.Pie(labels=labels,values=datavals)\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title = title,\n",
    "        height=600,\n",
    "        width=600\n",
    "        )\n",
    "    \n",
    "    fig = go.Figure(data=[trace], layout=layout)\n",
    "    iplot(fig)\n",
    "    \n",
    "pie_count(df, 'user_location', 0.5, 'Number of tweets per location')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to check last one categorical feature - `source`. Lets see top 40 sources by the number of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = df['source'].value_counts().reset_index()\n",
    "ds.columns = ['source', 'count']\n",
    "ds = ds.sort_values(['count'])\n",
    "\n",
    "fig = px.bar(\n",
    "    ds.tail(40), \n",
    "    x=\"count\", \n",
    "    y=\"source\", \n",
    "    orientation='h', \n",
    "    title='Top 40 user sources by number of tweets', \n",
    "    width=800, \n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Additional features analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create new feature - `hashtags_count` that will show us how many hashtags in the current tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['hashtags'] = df['hashtags'].fillna('[]')\n",
    "df['hashtags_count'] = df['hashtags'].apply(lambda x: len(x))\n",
    "df.loc[df['hashtags'] == '[]', 'hashtags_count'] = 0\n",
    "\n",
    "df[['hashtags','hashtags_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see the values for new created column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['hashtags_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df, \n",
    "    x=df['hashtags_count'], \n",
    "    y=df['tweets_count'], \n",
    "    height=700,\n",
    "    width=700,\n",
    "    title='Total number of tweets for users and number of hashtags in every tweet'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of new feature over the number of tweets is expected - a lot of tweets with few number of hashtags and few tweets with huge number of hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = df['hashtags_count'].value_counts().reset_index()\n",
    "ds.columns = ['hashtags_count', 'count']\n",
    "ds = ds.sort_values(['count'])\n",
    "ds['hashtags_count'] = ds['hashtags_count'].astype(str) + ' tags'\n",
    "\n",
    "fig = px.bar(\n",
    "    ds, \n",
    "    x=\"count\", \n",
    "    y=\"hashtags_count\", \n",
    "    orientation='h', \n",
    "    title='Distribution of number of hashtags in tweets', \n",
    "    width=800, \n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see top 40 users that like to use hashtags a little bit more than others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = df[df['tweets_count']>10]\n",
    "ds = ds.groupby(['user_name', 'tweets_count'])['hashtags_count'].mean().reset_index()\n",
    "ds.columns = ['user', 'tweets_count', 'mean_count']\n",
    "ds = ds.sort_values(['mean_count'])\n",
    "\n",
    "fig = px.bar(\n",
    "    ds.tail(40), \n",
    "    x=\"mean_count\", \n",
    "    y=\"user\", \n",
    "    color='tweets_count',\n",
    "    orientation='h', \n",
    "    title='Top 40 users with higher mean number of hashtags (at least 10 tweets per user)', \n",
    "    width=800, \n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just split day and time into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date']) \n",
    "df = df.sort_values(['date'])\n",
    "df['day'] = df['date'].astype(str).str.split(' ', expand=True)[0]\n",
    "df['time'] = df['date'].astype(str).str.split(' ', expand=True)[1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique users per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = df.groupby(['day', 'user_name'])['hashtags_count'].count().reset_index()\n",
    "ds = ds.groupby(['day'])['user_name'].count().reset_index()\n",
    "ds.columns = ['day', 'number_of_users']\n",
    "ds['day'] = ds['day'].astype(str) + ':00:00:00'\n",
    "fig = px.bar(\n",
    "    ds, \n",
    "    x='day', \n",
    "    y=\"number_of_users\", \n",
    "    orientation='v',\n",
    "    title='Number of unique users per day', \n",
    "    width=800, \n",
    "    height=800\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are going to check how many tweets were for every day in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = df['day'].value_counts().reset_index()\n",
    "ds.columns = ['day', 'count']\n",
    "ds = ds.sort_values('count')\n",
    "ds['day'] = ds['day'].astype(str) + ':00:00:00'\n",
    "fig = px.bar(\n",
    "    ds, \n",
    "    x='count', \n",
    "    y=\"day\", \n",
    "    orientation='h',\n",
    "    title='Tweets distribution over days present in dataset', \n",
    "    width=800, \n",
    "    height=800\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets do the same but for hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['hour'] = df['date'].dt.hour\n",
    "ds = df['hour'].value_counts().reset_index()\n",
    "ds.columns = ['hour', 'count']\n",
    "ds['hour'] = 'Hour ' + ds['hour'].astype(str)\n",
    "fig = px.bar(\n",
    "    ds, \n",
    "    x=\"hour\", \n",
    "    y=\"count\", \n",
    "    orientation='v', \n",
    "    title='Tweets distribution over hours', \n",
    "    width=800\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets split hashtags into separate column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_hashtags(x): \n",
    "    return str(x).replace('[', '').replace(']', '').split(',')\n",
    "\n",
    "tweets_df = df.copy()\n",
    "tweets_df['hashtag'] = tweets_df['hashtags'].apply(lambda row : split_hashtags(row))\n",
    "tweets_df = tweets_df.explode('hashtag')\n",
    "tweets_df['hashtag'] = tweets_df['hashtag'].astype(str).str.lower().str.replace(\"'\", '').str.replace(\" \", '')\n",
    "tweets_df.loc[tweets_df['hashtag']=='', 'hashtag'] = 'NO HASHTAG'\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And show top 20 hashtags on tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = tweets_df['hashtag'].value_counts().reset_index()\n",
    "ds.columns = ['hashtag', 'count']\n",
    "ds = ds.sort_values(['count'])\n",
    "fig = px.bar(\n",
    "    ds.tail(20), \n",
    "    x=\"count\", \n",
    "    y='hashtag', \n",
    "    orientation='h', \n",
    "    title='Top 20 hashtags', \n",
    "    width=800, \n",
    "    height=700\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are going to calculate the length for every tweet in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['tweet_length'] = df['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    df, \n",
    "    x=\"tweet_length\", \n",
    "    nbins=80, \n",
    "    title='Tweet length distribution', \n",
    "    width=800,\n",
    "    height=700\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = df[df['tweets_count']>=10]\n",
    "ds = ds.groupby(['user_name', 'tweets_count'])['tweet_length'].mean().reset_index()\n",
    "ds.columns = ['user_name', 'tweets_count', 'mean_length']\n",
    "ds = ds.sort_values(['mean_length'])\n",
    "fig = px.bar(\n",
    "    ds.tail(40), \n",
    "    x=\"mean_length\", \n",
    "    y=\"user_name\", \n",
    "    color='tweets_count',\n",
    "    orientation='h', \n",
    "    title='Top 40 users with the longest average length of tweet (at least 10 tweets)', \n",
    "    width=800, \n",
    "    height=800\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = df[df['tweets_count']>=10]\n",
    "ds = ds.groupby(['user_name', 'tweets_count'])['tweet_length'].mean().reset_index()\n",
    "ds.columns = ['user_name', 'tweets_count', 'mean_length']\n",
    "ds = ds.sort_values(['mean_length'])\n",
    "fig = px.bar(\n",
    "    ds.head(40), \n",
    "    x=\"mean_length\", \n",
    "    y=\"user_name\", \n",
    "    color='tweets_count',\n",
    "    orientation='h', \n",
    "    title='Top 40 users with the shortest average length of tweet (at least 10 tweets)', \n",
    "    width=800, \n",
    "    height=800\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## Tweets text analysis\n",
    "\n",
    "#### Here we are going to check the `text` feature of the dataset.\n",
    "#### Lets see general wordcloud for this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_wordcloud(text_column, title):\n",
    "    words = ' '.join(text_column.dropna().astype(str))\n",
    "    cleaned_words = \" \".join([word for word in words.split()\n",
    "                                if 'http' not in word\n",
    "                                    and not word.startswith('@')\n",
    "                                    and word != 'RT'])\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=set(STOPWORDS), \n",
    "        max_words=50, \n",
    "        max_font_size=40, \n",
    "        random_state=666\n",
    "    ).generate(cleaned_words)\n",
    "\n",
    "    fig = plt.figure(1, figsize=(14,14))\n",
    "    plt.axis('off')\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "build_wordcloud(df['cleaned_text'], 'Revalent words in tweets for all dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see world clouds for top 3 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_name = ds.sort_values(['tweets_count'], ascending=False)['user_name'].tolist()[0]\n",
    "test_df = df[df['user_name']==user_name]\n",
    "build_wordcloud(test_df['cleaned_text'], 'Revalent words in tweets for {}'.format(user_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_name = ds.sort_values(['tweets_count'], ascending=False)['user_name'].tolist()[1]\n",
    "test_df = df[df['user_name']==user_name]\n",
    "build_wordcloud(test_df['cleaned_text'], 'Revalent words in tweets for {}'.format(user_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_name = ds.sort_values(['tweets_count'], ascending=False)['user_name'].tolist()[2]\n",
    "test_df = df[df['user_name']==user_name]\n",
    "build_wordcloud(test_df['cleaned_text'], 'Revalent words in tweets for {}'.format(user_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's also visualize WordCloud for user's description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "build_wordcloud(df['user_description'], 'Prevalent words in tweets for Blood Donors India')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "\n",
    "## Sentiment analysis\n",
    "\n",
    "### using Tfidf Vectorizer to get features and Kmeans clustering algotithm to split data into 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vec = TfidfVectorizer(stop_words=\"english\")\n",
    "# vec.fit(df['text'].values)\n",
    "# features = vec.transform(df['text'].values)\n",
    "\n",
    "# kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "# kmeans.fit(features)\n",
    "\n",
    "# res = kmeans.predict(features)\n",
    "# df['Cluster'] = res\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df[df['Cluster'] == 0].head(10)['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df[df['Cluster'] == 1].head(10)['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['Cluster'] == 2].head(10)['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('Number of samples for class 0: ', len(df[df['Cluster'] == 0]))\n",
    "# print('Number of samples for class 1: ', len(df[df['Cluster'] == 1]))\n",
    "# print('Number of samples for class 2: ', len(df[df['Cluster'] == 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     build_wordcloud(df[df['Cluster'] == i]['text'].tolist(), 'Wordcloud for cluster ' + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that cluster 0 and 1 contains more or less positive tweets, but cluster 2 contains tweets with information about new cases, reports and regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature = df['Cluster']\n",
    "# top_n_active_cities = feature.value_counts()\n",
    "# print(top_n_active_cities)\n",
    "\n",
    "# ncount = feature.shape[0]\n",
    "\n",
    "# plt.figure(figsize=(12,5))\n",
    "# chart = sns.countplot(feature, order=feature.value_counts().index)\n",
    "# plt.xticks(\n",
    "#     rotation=45, \n",
    "#     horizontalalignment='right',\n",
    "#     fontweight='light',\n",
    "#     fontsize='x-large' \n",
    "# )\n",
    "# chart.set_title('Sentiment Analysis of Tweets', fontsize=15)\n",
    "# chart.set_xlabel(\"Sentiment\",fontsize=20)\n",
    "# chart.set_ylabel(\"Counts (Number of Tweets)\",fontsize=20)\n",
    "\n",
    "# # Make twin axis\n",
    "# chart2 = chart.twinx()\n",
    "\n",
    "# # Switch so count axis is on right, frequency on left\n",
    "# chart2.yaxis.tick_left()\n",
    "# chart.yaxis.tick_right()\n",
    "\n",
    "# # Also switch the labels over\n",
    "# chart.yaxis.set_label_position('right')\n",
    "# chart2.yaxis.set_label_position('left')\n",
    "\n",
    "# chart2.set_ylabel('Frequency [%]',fontsize=20)\n",
    "\n",
    "# for p in chart.patches:\n",
    "#     x=p.get_bbox().get_points()[:,0]\n",
    "#     y=p.get_bbox().get_points()[1,1]\n",
    "#     chart.annotate('{:.1f}%'.format(100.*y/ncount), (x.mean(), y), \n",
    "#             ha='center', va='bottom') # set the alignment of the text\n",
    "\n",
    "# # Use a LinearLocator to ensure the correct number of ticks\n",
    "# chart.yaxis.set_major_locator(ticker.LinearLocator(11))\n",
    "\n",
    "# # Fix the frequency range to 0-100\n",
    "# chart2.set_ylim(0,100)\n",
    "# chart.set_ylim(0,ncount)\n",
    "\n",
    "# # And use a MultipleLocator to ensure a tick spacing of 10\n",
    "# chart2.yaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of COVID-19 Tweets using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentiment using TextBlob\n",
    "df[\"sentiment\"] = df[\"cleaned_text\"].dropna().parallel_apply(get_tweet_sentiment)\n",
    "\n",
    "df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving datafrme\n",
    "df.to_csv(\"covid_tweets_with_sentiments_{}.csv\".format(datetime.date.today()), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiments of Tweets\n",
    "The tweets having a positive, negative, or neutral sentiment have already been\n",
    "determined. Here, the coding is done to display this information pictorially using\n",
    "explode library, to make a pie chart to display this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def show_sentiment_analysis_barplot(dataframe, sentiment_column, country=\"all\"):\n",
    "    \n",
    "    if country != \"all\":\n",
    "        dataframe = dataframe[dataframe.country == country]\n",
    "        \n",
    "    feature = dataframe[sentiment_column]\n",
    "    top_n_active_cities = feature.value_counts()\n",
    "    print(top_n_active_cities)\n",
    "\n",
    "    ncount = feature.shape[0]\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    chart = sns.countplot(feature, order=feature.value_counts().index)\n",
    "    plt.xticks(\n",
    "        rotation=45, \n",
    "        horizontalalignment='right',\n",
    "        fontweight='light',\n",
    "        fontsize='x-large' \n",
    "    )\n",
    "    \n",
    "    if country == \"all\":\n",
    "        chart.set_title('Sentiment Analysis of {} Tweets in all countries'.format(ncount), fontsize=15)\n",
    "    else:\n",
    "        chart.set_title('Sentiment Analysis of {} Tweets in {}'.format(ncount,' '.join([name.capitalize() for name in country.split(\" \")])), fontsize=15)\n",
    "        \n",
    "    chart.set_xlabel(\"Sentiment\",fontsize=20)\n",
    "    chart.set_ylabel(\"Counts (Number of Tweets)\",fontsize=20)\n",
    "\n",
    "    # Make twin axis\n",
    "    chart2 = chart.twinx()\n",
    "\n",
    "    # Switch so count axis is on right, frequency on left\n",
    "    chart2.yaxis.tick_left()\n",
    "    chart.yaxis.tick_right()\n",
    "\n",
    "    # Also switch the labels over\n",
    "    chart.yaxis.set_label_position('right')\n",
    "    chart2.yaxis.set_label_position('left')\n",
    "\n",
    "    chart2.set_ylabel('Frequency [%]',fontsize=20)\n",
    "\n",
    "    for p in chart.patches:\n",
    "        x=p.get_bbox().get_points()[:,0]\n",
    "        y=p.get_bbox().get_points()[1,1]\n",
    "        chart.annotate('{:.1f}%'.format(100.*y/ncount), (x.mean(), y), \n",
    "                ha='center', va='bottom') # set the alignment of the text\n",
    "\n",
    "    # Use a LinearLocator to ensure the correct number of ticks\n",
    "    chart.yaxis.set_major_locator(ticker.LinearLocator(11))\n",
    "\n",
    "    # Fix the frequency range to 0-100\n",
    "    chart2.set_ylim(0,100)\n",
    "    chart.set_ylim(0,ncount)\n",
    "\n",
    "    # And use a MultipleLocator to ensure a tick spacing of 10\n",
    "    chart2.yaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "show_sentiment_analysis_barplot(dataframe=df, sentiment_column='sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation with geographical distribution of tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here I am going to show approach how to use plotly world map to demonstrate geographical distribution of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['location'] = df['user_location'].str.split(',', expand=True)[1].str.lstrip().str.rstrip()\n",
    "res = df.groupby(['location'])['cleaned_text'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "country_dict = {}\n",
    "for c in countries:\n",
    "    country_dict[c.name] = c.alpha3\n",
    "    \n",
    "res['alpha3'] = res['location']\n",
    "res = res.replace({\"alpha3\": country_dict})\n",
    "\n",
    "\n",
    "country_list =  tw['Country'].tolist()[:5]\n",
    "\n",
    "res = res[\n",
    "    (res['alpha3'] == 'USA') | \n",
    "    (res['location'].isin(country_list)) | \n",
    "    (res['location'] != res['alpha3'])\n",
    "]\n",
    "\n",
    "gbr = ['England', 'UK', 'London', 'United Kingdom']\n",
    "us = ['United States', 'NY', 'CA', 'GA']\n",
    "\n",
    "res = res[res['location'].notnull()]\n",
    "res.loc[res['location'].isin(gbr), 'alpha3'] = 'GBR'\n",
    "res.loc[res['location'].isin(us), 'alpha3'] = 'USA'\n",
    "res.loc[res['alpha3'] == 'USA', 'location'] = 'USA'\n",
    "res.loc[res['alpha3'] == 'GBR', 'location'] = 'United Kingdom'\n",
    "plot = res.groupby(['location', 'alpha3'])['cleaned_text'].sum().reset_index()\n",
    "plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.choropleth(\n",
    "    plot, \n",
    "    locations=\"alpha3\",\n",
    "    hover_name='location',\n",
    "    color=\"cleaned_text\",\n",
    "    projection=\"natural earth\",\n",
    "    color_continuous_scale=px.colors.sequential.Plasma,\n",
    "    title='Tweets from different countries for every day',\n",
    "    width=800, \n",
    "    height=600\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = df.groupby([ 'location', 'user_name'])['text'].count().reset_index()\n",
    "res = res[['location', 'user_name']]\n",
    "res['alpha3'] = res['location']\n",
    "res = res.replace({\"alpha3\": country_dict})\n",
    "\n",
    "country_list = tw['Country'].tolist()[:5]\n",
    "\n",
    "res = res[\n",
    "    (res['alpha3'] == 'USA') | \n",
    "    (res['location'].isin(country_list)) | \n",
    "    (res['location'] != res['alpha3'])\n",
    "]\n",
    "\n",
    "gbr = ['England', 'UK', 'London', 'United Kingdom']\n",
    "us = ['United States', 'NY', 'CA', 'GA']\n",
    "\n",
    "res = res[res['location'].notnull()]\n",
    "res.loc[res['location'].isin(gbr), 'alpha3'] = 'GBR'\n",
    "res.loc[res['location'].isin(us), 'alpha3'] = 'USA'\n",
    "res.loc[res['alpha3'] == 'USA', 'location'] = 'USA'\n",
    "res.loc[res['alpha3'] == 'GBR', 'location'] = 'United Kingdom'\n",
    "plot = res.groupby(['location', 'alpha3'])['user_name'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.choropleth(\n",
    "    plot, \n",
    "    locations=\"alpha3\",\n",
    "    hover_name='location',\n",
    "    color=\"user_name\",\n",
    "    projection=\"natural earth\",\n",
    "    color_continuous_scale=px.colors.sequential.Plasma,\n",
    "    title='Numbers of active users for every day',\n",
    "    width=800, \n",
    "    height=600\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['cleaned_text'].notna()]\n",
    "df['cleaned_text']= df['cleaned_text'].dropna().apply(lambda x: tweet_to_words(x))\n",
    "train_df, test_df = train_test_split(df, test_size=0.33,random_state=0)\n",
    "\n",
    "train_clean_tweets = train_df['cleaned_text'].tolist()\n",
    "test_clean_tweets = test_df['cleaned_text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get features for each set as matrix of words counts using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer = \"word\")\n",
    "train_features = count_vectorizer.fit_transform(train_clean_tweets)\n",
    "test_features = count_vectorizer.transform(test_clean_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifiers = [    \n",
    "    RandomForestClassifier(n_estimators=200, bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=50, max_features='auto', max_leaf_nodes=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_jobs= -1,\n",
    "            oob_score=False, random_state=10),\n",
    "    \n",
    "    AdaBoostClassifier(n_estimators=100, random_state=10),\n",
    "    \n",
    "    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
    "      \n",
    "    tree.DecisionTreeClassifier(),\n",
    "    \n",
    "    ensemble.ExtraTreesClassifier(n_estimators=100,\n",
    "                                  max_features= 50,\n",
    "                                  criterion= 'entropy'),\n",
    "\n",
    "    ensemble.GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.001,n_estimators=50, random_state=None, verbose = 0)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting and evaluating classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_features = train_features.toarray()\n",
    "dense_test = test_features.toarray()\n",
    "\n",
    "accuracies = []\n",
    "models = []\n",
    "\n",
    "for classifier in Classifiers:\n",
    "    try:\n",
    "        fit = classifier.fit(train_features, train_df['sentiment'])\n",
    "        pred = fit.predict(test_features)\n",
    "    except Exception:\n",
    "        fit = classifier.fit(dense_features, train_df['sentiment'])\n",
    "        pred = fit.predict(dense_test)\n",
    "\n",
    "    accuracy = accuracy_score(pred,test_df['sentiment'])\n",
    "    print(\"Classifier Name: {}\".format(classifier.__class__.__name__))\n",
    "    print('>> Accuracy : {}'.format(str(accuracy)))\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print(\">> Classification report for classifier {}: \\n {}\".format(classifier,\n",
    "                                                                     classification_report(test_df['sentiment'],\n",
    "                                                                     pred)))\n",
    "    cm = confusion_matrix(test_df['sentiment'], pred)\n",
    "    print(\">> Confusion matrix: \\n{}\".format(cm))\n",
    "    models.append(classifier.__class__.__name__)\n",
    "    print(\"=\" * 110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploting the Model Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"Model\":models, \"Accuracy\":accuracies})\n",
    "\n",
    "fig = px.bar(\n",
    "    data, \n",
    "    x=\"Model\",\n",
    "    y=\"Accuracy\", \n",
    "    orientation='v', \n",
    "    title='Accuracies of Models', \n",
    "    width=800, \n",
    "    height=600,\n",
    ")\n",
    "\n",
    "# Here we modify the tickangle of the xaxis, resulting in rotated labels.\n",
    "fig.update_layout(barmode='group', xaxis_tickangle=-45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get features using CountVectorizer and Ngrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer=\"word\", \n",
    "                                   ngram_range=(1,2), \n",
    "                                   min_df=3, \n",
    "                                   max_df=.95, \n",
    "                                   stop_words='english')\n",
    "train_features = count_vectorizer.fit_transform(train_clean_tweets)\n",
    "test_features = count_vectorizer.transform(test_clean_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting and evaluating classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_features = train_features.toarray()\n",
    "dense_test = test_features.toarray()\n",
    "\n",
    "accuracies = []\n",
    "models = []\n",
    "\n",
    "for classifier in Classifiers:\n",
    "    try:\n",
    "        fit = classifier.fit(train_features, train_df['sentiment'])\n",
    "        pred = fit.predict(test_features)\n",
    "    except Exception:\n",
    "        fit = classifier.fit(dense_features, train_df['sentiment'])\n",
    "        pred = fit.predict(dense_test)\n",
    "\n",
    "    accuracy = accuracy_score(pred,test_df['sentiment'])\n",
    "    print(\"Classifier Name: {}\".format(classifier.__class__.__name__))\n",
    "    print('>> Accuracy : {}'.format(str(accuracy)))\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print(\">> Classification report for classifier {}: \\n {}\".format(classifier,\n",
    "                                                                     classification_report(test_df['sentiment'],\n",
    "                                                                     pred)))\n",
    "    cm = confusion_matrix(test_df['sentiment'], pred)\n",
    "    print(\">> Confusion matrix: \\n{}\".format(cm))\n",
    "    models.append(classifier.__class__.__name__)\n",
    "    print(\"=\" * 110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploting the Model Performances after using Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"Model\":models, \"Accuracy\":accuracies})\n",
    "\n",
    "fig = px.bar(\n",
    "    data, \n",
    "    x=\"Model\",\n",
    "    y=\"Accuracy\", \n",
    "    orientation='v', \n",
    "    title='Accuracies of Models using Ngrams', \n",
    "    width=800, \n",
    "    height=600,\n",
    ")\n",
    "\n",
    "# Here we modify the tickangle of the xaxis, resulting in rotated labels.\n",
    "fig.update_layout(barmode='group', xaxis_tickangle=-45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiments Analysis of Specific Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"covid_tweets_with_sentiments_{}.csv\".format(datetime.date.today()))\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a module for text standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get topN similar sentences for a given sentence using Cosine Similarity\n",
    "def most_similar_sentence(sentence, all_sentences):\n",
    "\n",
    "#     print(\"Given sentence: \",sentence)\n",
    "    max_cosine_sim = 0.5\n",
    "    most_sim_sentence = ''\n",
    "    most_sim_sentence_index = 0\n",
    "    \n",
    "    for index, sim_sentence in enumerate(all_sentences):\n",
    "        cosine = Cosine(2)\n",
    "        s0 = sentence\n",
    "        s1 = sim_sentence\n",
    "        p0 = cosine.get_profile(s0)\n",
    "        p1 = cosine.get_profile(s1)\n",
    "        if p1 and cosine.similarity_profiles(p0, p1) > max_cosine_sim:\n",
    "            max_cosine_sim = cosine.similarity_profiles(p0, p1)\n",
    "            most_sim_sentence = sim_sentence\n",
    "            most_sim_sentence_index = index\n",
    "            \n",
    "    sentences_cosine_similarities = (s0,most_sim_sentence,max_cosine_sim)\n",
    "#     print(\"Most similar sentence: {}\".format(most_sim_sentence).encode('utf-8').strip())\n",
    "\n",
    "    return (most_sim_sentence, max_cosine_sim, most_sim_sentence_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = \"India\"\n",
    "\n",
    "sim_country = most_similar_sentence(country, df.country.dropna().unique().tolist())[0]\n",
    "show_sentiment_analysis_barplot(dataframe=df, sentiment_column='sentiment', country=sim_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_wordcloud(df[df['country'] == sim_country]['cleaned_text'],\n",
    "                'Wordcloud for {} Tweets'.format(' '.join([name.capitalize() for name in sim_country.split(\" \")])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_wordcloud(df[(df['country'] == sim_country) & (df['sentiment'] == 'positive')]['cleaned_text'],\n",
    "                'Wordcloud for {} Positive Tweets'.format(' '.join([name.capitalize() for name in sim_country.split(\" \")])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_wordcloud(df[(df['country'] == sim_country) & (df['sentiment'] == 'negative')]['cleaned_text'],\n",
    "                'Wordcloud for {} Negative Tweets'.format(' '.join([name.capitalize() for name in sim_country.split(\" \")])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "build_wordcloud(df[(df['country'] == sim_country) & (df['sentiment'] == 'neutral')]['cleaned_text'],\n",
    "                'Wordcloud for {} Neutral Tweets'.format(' '.join([name.capitalize() for name in sim_country.split(\" \")])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get covid daily cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import covid_daily\n",
    "\n",
    "covid_daily_df = covid_daily.overview(as_json=False)\n",
    "\n",
    "covid_daily_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### APIs for covid-19 cases stats. per country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_world_population_countries_names():\n",
    "    url = \"https://world-population.p.rapidapi.com/allcountriesname\"\n",
    "    headers = {\n",
    "        'x-rapidapi-key': \"134117ae79msh40bb2931f9c7e4ap1aa445jsn8d1982670b09\",\n",
    "        'x-rapidapi-host': \"world-population.p.rapidapi.com\"\n",
    "        }\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "    return response.json() \n",
    "\n",
    "\n",
    "def get_world_population_per_country(country):\n",
    "    url = \"https://world-population.p.rapidapi.com/population\"\n",
    "    querystring = {\"country_name\":country}\n",
    "    headers = {\n",
    "        'x-rapidapi-key': \"134117ae79msh40bb2931f9c7e4ap1aa445jsn8d1982670b09\",\n",
    "        'x-rapidapi-host': \"world-population.p.rapidapi.com\"\n",
    "        }\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_covid19api_all_countries():\n",
    "    url = \"https://api.covid19api.com/countries\"\n",
    "    payload={}\n",
    "    headers = {}\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload, timeout=10)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_covid19api_stats_country(country , from_date , to_date ):\n",
    "    url = \"https://api.covid19api.com/total/country/{}?from={}&to={}\".format(country , from_date , to_date )\n",
    "    payload={}\n",
    "    headers = {}\n",
    "    response = requests.request(\"GET\", url, headers=headers, data = payload )\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_covid_observer_countries():\n",
    "    url = \"https://covid.observer/us/#countries\"\n",
    "    response = requests.request(\"GET\", url, timeout=10)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    countries_list_element = soup.find_all(\"div\",{\"class\":\"countries-list\"})[2]\n",
    "    \n",
    "    result = {}\n",
    "    for a_tag in countries_list_element.find_all(\"a\"):\n",
    "        result[a_tag.text] = \"https://covid.observer\" + a_tag['href']\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get covid cases dataframe for specific country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get world countries\n",
    "all_countries = get_covid_observer_countries()\n",
    "\n",
    "# get most similar country\n",
    "sim_country = most_similar_sentence(country, all_countries.keys())[0]\n",
    "\n",
    "# get API url for the similar country\n",
    "url = all_countries[sim_country]\n",
    "\n",
    "# make a get request\n",
    "page = requests.get(url)\n",
    "\n",
    "# declare a BeautifulSoup object from request content\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "# text mining on BeautifulSoup object\n",
    "table_data = soup.find('table')\n",
    "headers = []\n",
    "for i in table_data.find_all('th'):\n",
    "    title = i.text\n",
    "    headers.append(title)\n",
    "\n",
    "# creatr a DataFrame\n",
    "specific_country_stat_df = pd.DataFrame(columns = headers)\n",
    "\n",
    "for j in table_data.find_all('tr')[1:]:\n",
    "    row_data = j.find_all('td')\n",
    "    row = [tr.text for tr in row_data]\n",
    "    length = len(specific_country_stat_df)\n",
    "    specific_country_stat_df.loc[length] = row\n",
    "        \n",
    "\n",
    "specific_country_stat_df.columns = ['date','confirmed_cases','daily_growth','recovered_cases',\n",
    "                                     'fatal_cases','active_cases','recovery_rate','mortality_rate',\n",
    "                                     'affected_population','confirmed_per_1000','died_per_1000']\n",
    "\n",
    "specific_country_stat_df['index'] = specific_country_stat_df.index\n",
    "\n",
    "specific_country_stat_df['country'] = sim_country\n",
    "\n",
    "specific_country_stat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing covid cases dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['confirmed_cases', 'recovered_cases', 'fatal_cases','active_cases']:\n",
    "    specific_country_stat_df[col] = specific_country_stat_df[col].parallel_apply(lambda x : int(x.replace(\",\",\"\")))\n",
    "    specific_country_stat_df[col] = specific_country_stat_df[col].astype(int)\n",
    "\n",
    "specific_country_stat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column for the date with year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [', 2021'] * specific_country_stat_df[specific_country_stat_df['date']=='Jan 1'].index[0]\n",
    "\n",
    "observations_num_in_2020 = len(specific_country_stat_df) - specific_country_stat_df[specific_country_stat_df['date']=='Jan 1'].index[0]\n",
    "print(observations_num_in_2020)\n",
    "\n",
    "y.extend([', 2020'] * observations_num_in_2020)\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_country_stat_df['year'] = y\n",
    "specific_country_stat_df['date_with_year'] = specific_country_stat_df['date'] + specific_country_stat_df['year']\n",
    "specific_country_stat_df['date_with_year'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the beginning and end of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beginning: \", pd.to_datetime(specific_country_stat_df['date_with_year']).min())\n",
    "print(\"End: \", pd.to_datetime(specific_country_stat_df['date_with_year']).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get covid vaccinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call API of covid vaccinations\n",
    "url = \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv\"\n",
    "page = requests.get(url)\n",
    "\n",
    "# save data into csv file\n",
    "vaccinations_file_path = 'vaccinations.csv'\n",
    "vaccinations_file = open(vaccinations_file_path, 'wb')\n",
    "vaccinations_file.write(page.content)\n",
    "vaccinations_file.close()\n",
    "\n",
    "# load csv as dataframe\n",
    "covid_vaccinations_df = pd.read_csv(vaccinations_file_path)\n",
    "covid_vaccinations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get covid vaccinations for specific country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_country = most_similar_sentence(specific_country_stat_df['country'].iloc[0], covid_vaccinations_df['location'].unique())[0]\n",
    "specific_covid_vaccinations_df = covid_vaccinations_df[covid_vaccinations_df.location == sim_country]\n",
    "specific_covid_vaccinations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the beginning and end of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beginning: \", pd.to_datetime(specific_covid_vaccinations_df['date']).min())\n",
    "print(\"End: \", pd.to_datetime(specific_covid_vaccinations_df['date']).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing dates to datetime\n",
    "specific_covid_vaccinations_df['date'] = pd.to_datetime(specific_covid_vaccinations_df['date'])\n",
    "specific_country_stat_df['date_with_year'] = pd.to_datetime(specific_country_stat_df['date_with_year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging covid cases and vaccinations dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_country_stat_vac_df = pd.merge(specific_covid_vaccinations_df,\n",
    "        specific_country_stat_df, \n",
    "        how='right',\n",
    "        left_on=[\"date\"],\n",
    "        right_on=[\"date_with_year\"])\n",
    "\n",
    "specific_country_stat_vac_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe for sentiments counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter tweets df by specific country\n",
    "specific_country_tweets_df = df[df.country == most_similar_sentence(country, df['country'].dropna().unique())[0]]\n",
    "\n",
    "# create a dataframe of groubing by sentiments counts per day\n",
    "sentiments_counts_df = pd.DataFrame({'count' : specific_country_tweets_df.groupby(['day','sentiment'])['sentiment'].count()}).reset_index()\n",
    "\n",
    "# create a pivot table for sentiments counts\n",
    "sentiments_counts_df = sentiments_counts_df.pivot(index=\"day\", columns=\"sentiment\", values=\"count\").reset_index()\n",
    "sentiments_counts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make cumulative sum for sentiments counts\n",
    "for col in ['negative','neutral','positive']:\n",
    "    sentiments_counts_df[col] = sentiments_counts_df[col].cumsum(axis = 0)\n",
    "\n",
    "sentiments_counts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse date of sentiments_counts_df to datetime\n",
    "sentiments_counts_df['day'] = pd.to_datetime(sentiments_counts_df['day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of intersected dates between specific_country_stat_vac_df and sentiments_counts_df: \",\n",
    "      len(set(specific_country_stat_vac_df['date_with_year'].unique()).intersection(set(sentiments_counts_df['day'].unique()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging all dataframes for specific country (sentiments counts, covid cases and vaccinations stats) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_country_sentiment_stat_vac_df = pd.merge(sentiments_counts_df,\n",
    "                                                  specific_country_stat_vac_df, \n",
    "                                                  how='right',\n",
    "                                                  left_on=[\"day\"],\n",
    "                                                  right_on=[\"date_with_year\"])\n",
    "specific_country_sentiment_stat_vac_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_country_sentiment_stat_vac_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter dataframe to have only rows whose date is in tweets dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_country_sentiment_stat_vac_df = specific_country_sentiment_stat_vac_df[specific_country_sentiment_stat_vac_df.date_with_year.isin(sentiments_counts_df.day)]\n",
    "specific_country_sentiment_stat_vac_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling numerical columns for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "for col in ['negative','neutral','positive','confirmed_cases', \n",
    "            'recovered_cases', 'fatal_cases', 'active_cases',\n",
    "            'total_vaccinations', 'people_fully_vaccinated']:\n",
    "    specific_country_sentiment_stat_vac_df[col] = min_max_scaler.fit_transform(specific_country_sentiment_stat_vac_df[[col]].values)\n",
    "\n",
    "specific_country_sentiment_stat_vac_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw a Multiple lines Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_stats_fig = go.Figure()\n",
    "\n",
    "for column in ['confirmed_cases', 'recovered_cases', 'fatal_cases', 'active_cases',\n",
    "               'total_vaccinations', 'people_fully_vaccinated',\n",
    "              'negative', 'neutral', 'positive']:\n",
    "    color_dict = {\n",
    "        \"confirmed_cases\": \"#118ab2\",\n",
    "        \"recovered_cases\": \"#ef476f\",\n",
    "        \"fatal_cases\": \"#06d6a0\",\n",
    "        \"active_cases\": \"#073b4c\",\n",
    "        \"total_vaccinations\": \" #979a9a\",\n",
    "        \"people_fully_vaccinated\": \" #979a9a\",\n",
    "        \"negative\": \" #641e16\",\n",
    "        \"neutral\": \" #641e16\",\n",
    "        \"positive\": \" #641e16\",\n",
    "        }\n",
    "    base_stats_fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = specific_country_sentiment_stat_vac_df['date_with_year'],\n",
    "            y = specific_country_sentiment_stat_vac_df[column],\n",
    "            name = column,\n",
    "            line = dict(color=color_dict[column]),\n",
    "            hovertemplate ='<br><b>Date</b>: %{x}'+'<br><i>Count</i>:'+'%{y}',\n",
    "        )\n",
    "    )\n",
    "    \n",
    "\n",
    "base_stats_fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "        buttons=list(\n",
    "            [dict(label = 'All Cases',\n",
    "                  method = 'update',\n",
    "                  args = [{'visible': [True, True, True, True, True, True, True, True, True, True, True, True]},\n",
    "                          {'title': 'All Cases',\n",
    "                           'showlegend':True}]),\n",
    "             \n",
    "             dict(label = 'active_cases',\n",
    "                  method = 'update',\n",
    "                  args = [{'visible': [False, False, False, True, False, False, False, False, False, False, False, False]},\n",
    "                          {'title': 'active_cases',\n",
    "                           'showlegend':True}]),\n",
    "             \n",
    "             dict(label = 'confirmed_cases',\n",
    "                  method = 'update',\n",
    "                  args = [{'visible': [True, False, False, False, False, False, False, False, False, False, False, False]},\n",
    "                          {'title': 'confirmed_cases',\n",
    "                           'showlegend':True}]),\n",
    "             \n",
    "             dict(label = 'recovered_cases',\n",
    "                  method = 'update',\n",
    "                  args = [{'visible': [False, True, False, False, False, False, False, False, False, False, False, False]},\n",
    "                          {'title': 'recovered_cases',\n",
    "                           'showlegend':True}]),\n",
    "\n",
    "             dict(label = 'fatal_cases',\n",
    "                  method = 'update',\n",
    "                  args = [{'visible': [False, False, True, False, False, False, False, False, False, False, False, False]},\n",
    "                          {'title': 'fatal_cases',\n",
    "                           'showlegend':True}]),\n",
    "             \n",
    "             dict(label = 'total_vaccinations',\n",
    "                  method = 'update',\n",
    "                  args = [{'visible': [False, False, False, False, True, False, False, False, False, False, False, False]},\n",
    "                          {'title': 'total_vaccinations',\n",
    "                           'showlegend':True}]),\n",
    "\n",
    "             dict(label = 'people_fully_vaccinated',\n",
    "                  method = 'update',\n",
    "                  args = [{'visible': [False, False, False, False, False, True, False, False, False, False, False, False]},\n",
    "                          {'title': 'people_fully_vaccinated',\n",
    "                           'showlegend':True}]),\n",
    "                          \n",
    "             dict(label = 'negative',\n",
    "                  method = 'update',\n",
    "                  args = [{'visible': [False, False, False, False, False, False, True, False, False, False, False, False]},\n",
    "                          {'title': 'negative',\n",
    "                           'showlegend':True}]),\n",
    "             \n",
    "             dict(label = 'neutral',\n",
    "                  method = 'update',\n",
    "                  args = [{'visible': [False, False, False, False, False, False, False, True, False, False, False, False]},\n",
    "                          {'title': 'neutral',\n",
    "                           'showlegend':True}]),\n",
    "\n",
    "             dict(label = 'positive',\n",
    "                  method = 'update',\n",
    "                  args = [{'visible': [False, False, False, False, False, False, False, False, True, False, False, False]},\n",
    "                          {'title': 'positive',\n",
    "                           'showlegend':True}]),\n",
    "\n",
    "            ]),\n",
    "             type = \"dropdown\",\n",
    "             direction=\"down\",\n",
    "             showactive=True,\n",
    "             x=0,\n",
    "             xanchor=\"left\",\n",
    "             y=1.25,\n",
    "             yanchor=\"top\"\n",
    "        ),\n",
    "        dict(\n",
    "        buttons=list(\n",
    "            [dict(label = 'Linear Scale',\n",
    "                  method = 'relayout',\n",
    "                  args = [{'yaxis': {'type': 'linear'}},\n",
    "                          {'title': 'All Cases',\n",
    "                           'showlegend':True}]),\n",
    "             dict(label = 'Log Scale',\n",
    "                  method = 'relayout',\n",
    "                  args = [{'yaxis': {'type': 'log'}},\n",
    "                          {'title': 'active_cases',\n",
    "                           'showlegend':True}]),\n",
    "            ]),\n",
    "             type = \"dropdown\",\n",
    "             direction=\"down\",\n",
    "             showactive=True,\n",
    "             x=0,\n",
    "             xanchor=\"left\",\n",
    "             y=1.36,\n",
    "             yanchor=\"top\"\n",
    "        )\n",
    "    ])\n",
    "\n",
    "base_stats_fig.update_xaxes(showticklabels=False)\n",
    "base_stats_fig.update_layout(\n",
    "    title_text=\"Statistics about Covid19 in {}\".format(country), title_x=0.5, title_font_size=20,\n",
    "    legend=dict(orientation='h', yanchor='top', y=1.15, xanchor='right', x=1), \n",
    "    paper_bgcolor=\"mintcream\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"# of Cases\")\n",
    "\n",
    "base_stats_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
